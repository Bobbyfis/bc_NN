{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ebf91-9250-417f-86ad-54ece7c8fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTransformer(L.LightningModule):\n",
    "    def __init__(self, lr=1e-3, d_model=64, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        \n",
    "        # project one-hot (20) to d_model dimensions\n",
    "        self.input_proj = nn.Linear(20, d_model)\n",
    "        \n",
    "        # transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=256,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # output projection: d_model â†’ 20 (one ddG per possible mutation)\n",
    "        self.output_proj = nn.Linear(d_model, 20)\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (1, L, 20)\n",
    "        out = self.input_proj(x)      # (1, L, d_model)\n",
    "        out = self.transformer(out)   # (1, L, d_model)\n",
    "        out = self.output_proj(out)   # (1, L, 20)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x      = batch['sequence']   # (1, L, 20)\n",
    "        mask   = batch['mask']       # (1, L, 20)\n",
    "        target = batch['labels']     # (1, L, 20)\n",
    "        pred   = self(x)             # (1, L, 20)\n",
    "        loss   = self.loss_fn(pred[mask==1], target[mask==1])\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x      = batch['sequence']\n",
    "        mask   = batch['mask']\n",
    "        target = batch['labels']\n",
    "        pred   = self(x)\n",
    "        loss   = self.loss_fn(pred[mask==1], target[mask==1])\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d944d-4e5e-4e09-a9fa-b073406ddaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use notebook 3's dataloader\n",
    "dataset_train = SequenceData('data/mega_train.csv')\n",
    "dataset_val   = SequenceData('data/mega_val.csv')\n",
    "loader_train  = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
    "loader_val    = DataLoader(dataset_val,   batch_size=1, shuffle=False)\n",
    "\n",
    "model   = SequenceTransformer(lr=1e-3, d_model=64, nhead=4, num_layers=2)\n",
    "trainer = L.Trainer(devices=1, max_epochs=20)\n",
    "trainer.fit(model, loader_train, loader_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
