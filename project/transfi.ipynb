{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f1ebf91-9250-417f-86ad-54ece7c8fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import time\n",
    "import lightning as L\n",
    "import os\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import sklearn.metrics as skmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "aa_alphabet = 'ACDEFGHIKLMNPQRSTVWY' # amino acid alphabet\n",
    "aa_to_int = {aa: i for i, aa in enumerate(aa_alphabet)} # mapping from amino acid to number\n",
    "\n",
    "# function to one hot encode sequence\n",
    "def one_hot_encode(sequence):\n",
    "    # initialize a zero matrix of shape (len(sequence), len(amino_acids))\n",
    "    one_hot = torch.zeros(len(sequence), len(aa_alphabet))\n",
    "    for i, aa in enumerate(sequence):\n",
    "        # set the column corresponding to the amino acid to 1\n",
    "        one_hot[i].scatter_(0, torch.tensor([aa_to_int[aa]]), 1)\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "from torchmetrics.regression import PearsonCorrCoef, SpearmanCorrCoef\n",
    "    \n",
    "class SequenceTransformer(L.LightningModule):\n",
    "    def __init__(self, lr=1e-3, d_model=64, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        \n",
    "        # project one-hot (20) to d_model dimensions\n",
    "        self.input_proj = nn.Linear(20, d_model)\n",
    "        \n",
    "        # transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=256,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # output projection: d_model â†’ 20 (one ddG per possible mutation)\n",
    "        self.output_proj = nn.Linear(d_model, 20)\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        # Metrics\n",
    "        self.val_pearson = PearsonCorrCoef()\n",
    "        self.val_spearman = SpearmanCorrCoef()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (1, L, 20) - NO squeeze here\n",
    "        out = self.input_proj(x)      # (1, L, d_model)\n",
    "        out = self.transformer(out)   # (1, L, d_model)\n",
    "        out = self.output_proj(out)   # (1, L, 20)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x      = batch['sequence'].squeeze(1)  # squeeze dim 1 not 0\n",
    "        mask   = batch['mask'].squeeze(1)\n",
    "        target = batch['labels'].squeeze(1)\n",
    "        pred   = self(x)\n",
    "        # flatten before masking\n",
    "        pred_flat   = pred.reshape(-1)  \n",
    "        target_flat = target.reshape(-1)\n",
    "        mask_flat   = mask.reshape(-1)\n",
    "        loss = self.loss_fn(pred_flat[mask_flat==1], target_flat[mask_flat==1])\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x      = batch['sequence'].squeeze(1)\n",
    "        mask   = batch['mask'].squeeze(1)\n",
    "        target = batch['labels'].squeeze(1)\n",
    "        pred   = self(x)\n",
    "        pred_flat   = pred.reshape(-1)\n",
    "        target_flat = target.reshape(-1)\n",
    "        mask_flat   = mask.reshape(-1)\n",
    "        \n",
    "        pred_masked   = pred_flat[mask_flat==1]\n",
    "        target_masked = target_flat[mask_flat==1]\n",
    "        \n",
    "        loss = self.loss_fn(pred_masked, target_masked)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        return {'loss': loss, 'preds': pred_masked.detach(), 'targets': target_masked.detach()}\n",
    "    \n",
    "    def on_validation_epoch_end(self, outputs):\n",
    "        # concatenate all predictions and targets across batches\n",
    "        preds   = torch.cat([o['preds']   for o in outputs]).cpu().numpy()\n",
    "        targets = torch.cat([o['targets'] for o in outputs]).cpu().numpy()\n",
    "        \n",
    "        pearson  = scipy.stats.pearsonr(preds, targets)[0]\n",
    "        spearman = scipy.stats.spearmanr(preds, targets)[0]\n",
    "        \n",
    "        self.log('val_pearson',  pearson,  prog_bar=True, on_epoch=True)\n",
    "        self.log('val_spearman', spearman, prog_bar=True, on_epoch=True)\n",
    "\n",
    "         \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr = self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8c914132-e04d-473e-872c-b6942bd77ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceData(Dataset):\n",
    "    def __init__(self, csv_file, label_col=\"ddG_ML\"):\n",
    "        self.label_col = label_col\n",
    "        self.df = pd.read_csv(csv_file, sep=\",\")\n",
    "        self.df = self.df[self.df.mut_type != \"wt\"]\n",
    "        self.df[\"mutation_pos\"] = self.df[\"mut_type\"].apply(lambda x: int(x[1:-1])-1)\n",
    "        self.df[\"mutation_to\"]  = self.df[\"mut_type\"].apply(lambda x: aa_to_int[x[-1]])\n",
    "        self.df = self.df.groupby(\"WT_name\").agg(list)\n",
    "        self.wt_names = self.df.index.values\n",
    "\n",
    "        # preload everything into RAM\n",
    "        print(\"Loading sequences into RAM...\")\n",
    "        self.data = []\n",
    "        for wt_name in self.wt_names:\n",
    "            mut_row  = self.df.loc[wt_name]\n",
    "            seq      = mut_row[\"wt_seq\"][0]\n",
    "            seq_enc  = one_hot_encode(seq)\n",
    "            L        = len(seq_enc)\n",
    "\n",
    "            mask   = torch.zeros((1, L, 20))\n",
    "            target = torch.zeros((1, L, 20))\n",
    "\n",
    "            positions   = torch.tensor(mut_row[\"mutation_pos\"])\n",
    "            amino_acids = torch.tensor(mut_row[\"mutation_to\"])\n",
    "            labels      = torch.tensor(mut_row[label_col])\n",
    "\n",
    "            for i in range(L):\n",
    "                mask[0,   i, amino_acids[positions==i]] = 1\n",
    "                target[0, i, amino_acids[positions==i]] = labels[positions==i]\n",
    "\n",
    "            self.data.append({\n",
    "                \"sequence\": seq_enc[None,:,:].float(),\n",
    "                \"mask\":     mask,\n",
    "                \"labels\":   target\n",
    "            })\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d944d-4e5e-4e09-a9fa-b073406ddaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use notebook 3's dataloader\n",
    "dataset_train = SequenceData('data/mega_train.csv')\n",
    "dataset_val   = SequenceData('data/mega_val.csv')\n",
    "loader_train  = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
    "loader_val    = DataLoader(dataset_val,   batch_size=1, shuffle=False)\n",
    "\n",
    "model   = SequenceTransformer(lr=1e-5, d_model=64, nhead=4, num_layers=2)\n",
    "trainer = L.Trainer(devices=1, max_epochs=20)\n",
    "trainer.fit(model, loader_train, loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49da7df-5006-4aa9-82d4-65a3da83afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d14a68-ad7d-408f-bfd0-8dd61ce600be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
