{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea1c10-bc7b-4c3f-9b03-88822f353711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initialization ---\n",
      "Using compute device: cuda\n",
      "Loading dataset from data/mega_train.csv...\n",
      "Loaded 216919 samples.\n",
      "Loading dataset from data/mega_val.csv...\n",
      "Loaded 27481 samples.\n",
      "Loading pre-trained ESM model: facebook/esm2_t6_8M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "   Epoch 1 | Batch 500/3390 | Current Batch MSE: 0.7536\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import time\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. PyTorch Dataset with ESM Tokenizer\n",
    "# -------------------------------------------------------------------\n",
    "class ProteinStabilityESMDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_len=120):\n",
    "        print(f\"Loading dataset from {csv_file}...\")\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        print(f\"Loaded {len(self.data)} samples.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        wt_seq = row['wt_seq']\n",
    "        mut_seq = row['aa_seq']\n",
    "        ddg = row['ddG_ML']\n",
    "        \n",
    "        # Tokenize WT\n",
    "        wt_encoded = self.tokenizer(\n",
    "            wt_seq, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize Mutant\n",
    "        mut_encoded = self.tokenizer(\n",
    "            mut_seq, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        target = torch.tensor(ddg, dtype=torch.float32)\n",
    "        \n",
    "        # Squeeze to remove the batch dimension added by the tokenizer\n",
    "        return {\n",
    "            'wt_input_ids': wt_encoded['input_ids'].squeeze(0),\n",
    "            'wt_attention_mask': wt_encoded['attention_mask'].squeeze(0),\n",
    "            'mut_input_ids': mut_encoded['input_ids'].squeeze(0),\n",
    "            'mut_attention_mask': mut_encoded['attention_mask'].squeeze(0),\n",
    "            'target': target\n",
    "        }\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. The Siamese ESM Model\n",
    "# -------------------------------------------------------------------\n",
    "class SiameseESM(nn.Module):\n",
    "    def __init__(self, model_name='facebook/esm2_t6_8M_UR50D', freeze_esm=True):\n",
    "        super(SiameseESM, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ESM-2 Model\n",
    "        print(f\"Loading pre-trained ESM model: {model_name}\")\n",
    "        self.esm = EsmModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze the ESM layers so we don't destroy pre-trained biology knowledge\n",
    "        # and to save massive amounts of RAM/compute during training\n",
    "        if freeze_esm:\n",
    "            for param in self.esm.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # ESM hidden size (320 for the 8M model)\n",
    "        hidden_size = self.esm.config.hidden_size\n",
    "        \n",
    "        # Prediction Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, wt_ids, wt_mask, mut_ids, mut_mask):\n",
    "        # Pass WT through ESM\n",
    "        wt_outputs = self.esm(input_ids=wt_ids, attention_mask=wt_mask)\n",
    "        # Pass Mutant through ESM\n",
    "        mut_outputs = self.esm(input_ids=mut_ids, attention_mask=mut_mask)\n",
    "        \n",
    "        # Extract the [CLS] token representation (the 0th token) \n",
    "        # which acts as a summary of the entire sequence\n",
    "        wt_cls = wt_outputs.last_hidden_state[:, 0, :]\n",
    "        mut_cls = mut_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Calculate the biological difference\n",
    "        diff = mut_cls - wt_cls\n",
    "        \n",
    "        # Predict ddG\n",
    "        ddg_prediction = self.fc(diff)\n",
    "        return ddg_prediction.squeeze(1)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Training and Validation Loop\n",
    "# -------------------------------------------------------------------\n",
    "def train_and_evaluate():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"--- Initialization ---\")\n",
    "    print(f\"Using compute device: {device}\")\n",
    "    \n",
    "    # 1. Setup ESM Tokenizer\n",
    "    model_name = 'facebook/esm2_t6_8M_UR50D'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # 2. Load Data \n",
    "    train_dataset = ProteinStabilityESMDataset('data/mega_train.csv', tokenizer, max_len=120)\n",
    "    val_dataset = ProteinStabilityESMDataset('data/mega_val.csv', tokenizer, max_len=120)\n",
    "    \n",
    "    # Smaller batch size compared to LSTM because Transformers use more RAM\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # 3. Initialize Model\n",
    "    model = SiameseESM(model_name=model_name, freeze_esm=True).to(device)\n",
    "    criterion = nn.MSELoss() \n",
    "    \n",
    "    # Only pass the unfrozen parameters (the FC head) to the optimizer\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        lr=0.001\n",
    "    )\n",
    "    \n",
    "    epochs = 10\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Move data to GPU/CPU\n",
    "            wt_ids = batch['wt_input_ids'].to(device)\n",
    "            wt_mask = batch['wt_attention_mask'].to(device)\n",
    "            mut_ids = batch['mut_input_ids'].to(device)\n",
    "            mut_mask = batch['mut_attention_mask'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(wt_ids, wt_mask, mut_ids, mut_mask)\n",
    "            loss = criterion(predictions, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Print intermediate progress\n",
    "            if (batch_idx + 1) % 500 == 0:\n",
    "                print(f\"   Epoch {epoch+1} | Batch {batch_idx+1}/{len(train_loader)} | Current Batch MSE: {loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # --- VALIDATION PHASE ---\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                wt_ids = batch['wt_input_ids'].to(device)\n",
    "                wt_mask = batch['wt_attention_mask'].to(device)\n",
    "                mut_ids = batch['mut_input_ids'].to(device)\n",
    "                mut_mask = batch['mut_attention_mask'].to(device)\n",
    "                target = batch['target'].to(device)\n",
    "                \n",
    "                predictions = model(wt_ids, wt_mask, mut_ids, mut_mask)\n",
    "                loss = criterion(predictions, target)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                all_preds.extend(predictions.cpu().numpy())\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                \n",
    "        # Calculate Metrics\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        rmse = np.sqrt(avg_val_loss)\n",
    "        \n",
    "        pcc, _ = pearsonr(all_targets, all_preds)\n",
    "        scc, _ = spearmanr(all_targets, all_preds)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # --- EXTENSIVE EPOCH REPORT ---\n",
    "        print(f\"\\n==================================================\")\n",
    "        print(f\" Epoch {epoch+1}/{epochs} Completed in {epoch_time:.1f} seconds\")\n",
    "        print(f\"--------------------------------------------------\")\n",
    "        print(f\" LOSS:\")\n",
    "        print(f\"   -> Train MSE: {avg_train_loss:.4f}\")\n",
    "        print(f\"   -> Val MSE:   {avg_val_loss:.4f}\")\n",
    "        print(f\"   -> Val RMSE:  {rmse:.4f} kcal/mol\")\n",
    "        print(f\" METRICS:\")\n",
    "        print(f\"   -> Pearson Corr (PCC):  {pcc:.4f}\")\n",
    "        print(f\"   -> Spearman Corr (SCC): {scc:.4f}\")\n",
    "        print(f\"==================================================\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd9175-bffe-4ce2-9997-1b2124fe9741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
