{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a03e98-c142-47fd-81a4-f107c3ecd69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import time\n",
    "import lightning as L\n",
    "import os\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import sklearn.metrics as skmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee2c089e-4958-484c-9189-72168e49fdf2",
   "metadata": {},
   "source": [
    "class ProtEmbeddingDiffDataset(Dataset):\n",
    "    def __init__(self, tensor_folder, csv_file):\n",
    "        self.tensor_folder = tensor_folder\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # store wt embeddings lookup: WT_name â†’ wt embedding filename\n",
    "        self.wt_df = self.df[self.df.mut_type == 'wt']\n",
    "        # create dict: WT_name â†’ name (filename of wt embedding)\n",
    "        self.wt_lookup = dict(zip(self.wt_df['WT_name'], self.wt_df['name']))\n",
    "        \n",
    "        # only keep mutation rows\n",
    "        self.df = self.df[self.df.mut_type != 'wt'].reset_index(drop=True)\n",
    "        self.labels   = torch.tensor(self.df['ddG_ML'].values)\n",
    "        self.ids      = self.df['name'].values\n",
    "        self.wt_names = self.df['WT_name'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load mutant embedding\n",
    "        mut_path = os.path.join(self.tensor_folder, self.ids[idx] + '.pt')\n",
    "        mut_emb  = torch.load(mut_path)['mean_representations'][6]\n",
    "\n",
    "        # load wildtype embedding using lookup\n",
    "        wt_filename = self.wt_lookup[self.wt_names[idx]]\n",
    "        wt_path     = os.path.join(self.tensor_folder, wt_filename + '.pt')\n",
    "        wt_emb      = torch.load(wt_path)['mean_representations'][6]\n",
    "\n",
    "        # difference\n",
    "        diff = wt_emb - mut_emb\n",
    "\n",
    "        # concatenate mut + wt + diff â†’ (2304,)\n",
    "        #combined = torch.cat([mut_emb, wt_emb, diff], dim=0)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return diff, label.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0da49b-3c0c-4cd7-8c1f-2bc6bfd01053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtEmbeddingDiffDataset(Dataset):\n",
    "    def __init__(self, tensor_folder, csv_file):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # build wt lookup: WT_name â†’ wt embedding filename\n",
    "        wt_df = self.df[self.df.mut_type == 'wt']\n",
    "        wt_lookup = dict(zip(wt_df['WT_name'], wt_df['name']))\n",
    "        \n",
    "        # only keep mutation rows\n",
    "        self.df = self.df[self.df.mut_type != 'wt'].reset_index(drop=True)\n",
    "        self.labels   = torch.tensor(self.df['ddG_ML'].values)\n",
    "\n",
    "        # preload all embeddings into RAM\n",
    "        print(\"Loading embeddings into RAM...\")\n",
    "        self.embeddings = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            # mutant embedding\n",
    "            mut_emb = torch.load(os.path.join(tensor_folder, row['name'] + '.pt'))['mean_representations'][6]\n",
    "            # wildtype embedding\n",
    "            wt_emb  = torch.load(os.path.join(tensor_folder, wt_lookup[row['WT_name']] + '.pt'))['mean_representations'][6]\n",
    "            # difference\n",
    "            diff = mut_emb - wt_emb\n",
    "            # concatenate â†’ (2304,)\n",
    "            self.embeddings.append(torch.cat([mut_emb, wt_emb, diff], dim=0))\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a09dd400-37a5-46ca-8b79-40af9677a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ProtEmbeddingDiffDataset(\n",
    "    'data/mega_train_embeddings',\n",
    "    'data/mega_train.csv'\n",
    ")\n",
    "dataset_val = ProtEmbeddingDiffDataset(\n",
    "    'data/mega_val_embeddings',\n",
    "    'data/mega_val.csv'\n",
    ")\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=1024, shuffle=True,  num_workers=4)\n",
    "loader_val   = DataLoader(dataset_val,   batch_size=512,  shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0a53664-4ce4-4203-a9ec-197f773ff330",
   "metadata": {},
   "source": [
    "\n",
    "class StabModel(L.LightningModule):\n",
    "    def __init__(self, input_dim=768, lr=1e-3):\n",
    "        super().__init__()\n",
    "        # Adding a hidden layer to actually use the ReLU activation\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, input_dim]\n",
    "        return self.model(x).squeeze(1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a24535f0-dde1-439e-bba6-42ac864b79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchmetrics.regression import PearsonCorrCoef, SpearmanCorrCoef\n",
    "\n",
    "class StabModel(L.LightningModule):\n",
    "    def __init__(self, input_dim=768, lr=1e-2):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.Sigmoid(),\n",
    "         \n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.lr = lr\n",
    "\n",
    "        # Metrics\n",
    "        self.val_pearson = PearsonCorrCoef()\n",
    "        self.val_spearman = SpearmanCorrCoef()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        \n",
    "        # Print Train Loss to console/progress bar\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        \n",
    "        # Update correlation metrics\n",
    "        self.val_pearson(preds, y)\n",
    "        self.val_spearman(preds, y)\n",
    "        \n",
    "        # Print Val stats to console/progress bar\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_pear\", self.val_pearson, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_spear\", self.val_spearman, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ad72a-c4b5-4a2a-a1ac-ef85476f8458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "ðŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type             | Params | Mode  | FLOPs\n",
      "------------------------------------------------------------------\n",
      "0 | model        | Sequential       | 197 K  | train | 0    \n",
      "1 | loss_fn      | MSELoss          | 0      | train | 0    \n",
      "2 | val_pearson  | PearsonCorrCoef  | 0      | train | 0    \n",
      "3 | val_spearman | SpearmanCorrCoef | 0      | train | 0    \n",
      "------------------------------------------------------------------\n",
      "197 K     Trainable params\n",
      "0         Non-trainable params\n",
      "197 K     Total params\n",
      "0.788     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cc773826374201a1ba88ad0815f9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bc6a6d71c741ecbba504a359d02eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3eebe44455e4088b454364789302a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcb064cc18b4a078324430a7015f4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1624b324a02f41d18ab1054d54baf052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290ad1d030de42778c3639e494c85d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = StabModel(lr = 1e-3)\n",
    "\n",
    "trainer = L.Trainer(devices =1, max_epochs= 5)\n",
    "trainer.fit(model, loader_train, loader_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce5dd5e-278c-4f45-b504-f24bf15b150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743936cb-e739-4634-acda-b4c00c0faf29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
