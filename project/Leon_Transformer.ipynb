{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb3591-794b-4d3a-8005-5410d6159597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import time\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Tokenizer\n",
    "# -------------------------------------------------------------------\n",
    "AMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "AA_TO_IDX = {aa: i+1 for i, aa in enumerate(AMINO_ACIDS)}\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 21\n",
    "\n",
    "def encode_sequence(seq, max_len=120):\n",
    "    encoded = [AA_TO_IDX.get(aa, UNK_IDX) for aa in seq]\n",
    "    if len(encoded) < max_len:\n",
    "        encoded = encoded + [PAD_IDX] * (max_len - len(encoded))\n",
    "    else:\n",
    "        encoded = encoded[:max_len]\n",
    "    return encoded\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. PyTorch Dataset\n",
    "# -------------------------------------------------------------------\n",
    "class ProteinStabilityDataset(Dataset):\n",
    "    def __init__(self, csv_file, max_len=120):\n",
    "        print(f\"Loading dataset from {csv_file}...\")\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.max_len = max_len\n",
    "        print(f\"Loaded {len(self.data)} samples.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        wt_seq = row['wt_seq']\n",
    "        mut_seq = row['aa_seq']\n",
    "        ddg = row['ddG_ML']\n",
    "        \n",
    "        wt_encoded = torch.tensor(encode_sequence(wt_seq, self.max_len), dtype=torch.long)\n",
    "        mut_encoded = torch.tensor(encode_sequence(mut_seq, self.max_len), dtype=torch.long)\n",
    "        target = torch.tensor(ddg, dtype=torch.float32)\n",
    "        \n",
    "        return wt_encoded, mut_encoded, target\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Positional Encoding\n",
    "# -------------------------------------------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        # Reshape to (max_len, d_model) to easily add to batch_first tensors\n",
    "        self.register_buffer('pe', pe.squeeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:x.size(1), :].unsqueeze(0)\n",
    "        return self.dropout(x)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. The Siamese Transformer Model\n",
    "# -------------------------------------------------------------------\n",
    "class SiameseTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=22, d_model=128, nhead=4, num_layers=3, dim_feedforward=256, max_len=120):\n",
    "        super(SiameseTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 1. Embedding & Positional Encoding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_len)\n",
    "        \n",
    "        # 2. Transformer Encoder Blocks\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 3. Prediction Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward_one_branch(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Create a padding mask: True where the token is PAD_IDX\n",
    "        # This tells the attention mechanism to ignore these positions\n",
    "        padding_mask = (x == PAD_IDX)\n",
    "        \n",
    "        # Embed and scale, then add positional encoding\n",
    "        embedded = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        \n",
    "        # Pass through Transformer\n",
    "        transformer_out = self.transformer_encoder(embedded, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Global Average Pooling: Average the sequence dimension, ignoring pads\n",
    "        # We create a mask of 1s (valid) and 0s (pads)\n",
    "        mask = (~padding_mask).float().unsqueeze(-1)\n",
    "        # Sum all valid token embeddings\n",
    "        sum_embeddings = (transformer_out * mask).sum(dim=1)\n",
    "        # Divide by the number of valid tokens to get the mean\n",
    "        mean_pooled = sum_embeddings / mask.sum(dim=1).clamp(min=1e-9)\n",
    "        \n",
    "        return mean_pooled\n",
    "\n",
    "    def forward(self, wt_seq, mut_seq):\n",
    "        wt_features = self.forward_one_branch(wt_seq)\n",
    "        mut_features = self.forward_one_branch(mut_seq)\n",
    "        \n",
    "        # Siamese difference\n",
    "        diff = mut_features - wt_features\n",
    "        \n",
    "        ddg_prediction = self.fc(diff)\n",
    "        return ddg_prediction.squeeze(1)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Training and Validation Loop\n",
    "# -------------------------------------------------------------------\n",
    "def train_and_evaluate():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"--- Initialization ---\")\n",
    "    print(f\"Using compute device: {device}\")\n",
    "    \n",
    "    # Load Data (Update path to 'data/...' if in a subfolder)\n",
    "    train_dataset = ProteinStabilityDataset('data/mega_train.csv', max_len=120)\n",
    "    val_dataset = ProteinStabilityDataset('data/mega_val.csv', max_len=120)\n",
    "    \n",
    "    # Transformers use more VRAM than LSTMs, so batch size is set to 128\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "    \n",
    "    # Initialize Model (From-scratch Custom Transformer)\n",
    "    model = SiameseTransformer(d_model=128, nhead=4, num_layers=3).to(device)\n",
    "    criterion = nn.MSELoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005) # Slightly lower LR for transformers\n",
    "    \n",
    "    epochs = 10\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (wt, mut, target) in enumerate(train_loader):\n",
    "            wt, mut, target = wt.to(device), mut.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(wt, mut)\n",
    "            loss = criterion(predictions, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 200 == 0:\n",
    "                print(f\"   Epoch {epoch+1} | Batch {batch_idx+1}/{len(train_loader)} | Current Batch MSE: {loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # --- VALIDATION PHASE ---\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for wt, mut, target in val_loader:\n",
    "                wt, mut, target = wt.to(device), mut.to(device), target.to(device)\n",
    "                \n",
    "                predictions = model(wt, mut)\n",
    "                loss = criterion(predictions, target)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                all_preds.extend(predictions.cpu().numpy())\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                \n",
    "        # Calculate Metrics\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        rmse = np.sqrt(avg_val_loss)\n",
    "        \n",
    "        pcc, _ = pearsonr(all_targets, all_preds)\n",
    "        scc, _ = spearmanr(all_targets, all_preds)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # --- EXTENSIVE EPOCH REPORT ---\n",
    "        print(f\"\\n==================================================\")\n",
    "        print(f\" Epoch {epoch+1}/{epochs} Completed in {epoch_time:.1f} seconds\")\n",
    "        print(f\"--------------------------------------------------\")\n",
    "        print(f\" LOSS:\")\n",
    "        print(f\"   -> Train MSE: {avg_train_loss:.4f}\")\n",
    "        print(f\"   -> Val MSE:   {avg_val_loss:.4f}\")\n",
    "        print(f\"   -> Val RMSE:  {rmse:.4f} kcal/mol\")\n",
    "        print(f\" METRICS:\")\n",
    "        print(f\"   -> Pearson Corr (PCC):  {pcc:.4f}\")\n",
    "        print(f\"   -> Spearman Corr (SCC): {scc:.4f}\")\n",
    "        print(f\"==================================================\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
